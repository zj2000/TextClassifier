from jieba import posseg
from collections import Counter
import time
import math
import os
import codecs  # ç¼–ç è½¬æ¢
from sklearn.datasets.base import Bunch

stopWords = dict()
featureWords = list()  # featureWords[type][word] = P(word|type)
wholeWF = list()  # wholeWF[type][word] = è¯¥ç±»å‹çš„wordè¯é¢‘
wholeDF = list()  # wholeDF[type][word] = è¯¥ç±»å‹wordè¢«å¼•ç”¨çš„æ–‡æ¡£æ•°
totalWordsCount = list()
wholeWF.append(Counter())
wholeDF.append(dict())
featureWords.append(dict())
totalWordsCount.append(0)
# è¯„ä»·ç®—æ³•
TP = list()  # TP[type] è¡¨ç¤ºåˆ†ç±»å™¨è¯†åˆ«æ­£ç¡®ï¼Œåˆ†ç±»å™¨è®¤ä¸ºè¯¥æ ·æœ¬ä¸ºtype
FP = list()  # FP[type] åˆ†ç±»å™¨è¯†åˆ«ç»“æœé”™è¯¯ï¼Œåˆ†ç±»å™¨è®¤ä¸ºè¯¥æ ·æœ¬ä¸ºtype, å®é™…ä¸Šè¯¥æ ·æœ¬ä¸æ˜¯type
FN = list()  # FN[type] åˆ†ç±»å™¨è¯†åˆ«ç»“æœé”™è¯¯ï¼Œåˆ†ç±»å™¨è®¤ä¸ºè¯¥æ ·æœ¬ä¸æ˜¯type, å®é™…ä¸Šè¯¥æ ·æœ¬æ˜¯type
resultMatrix = list()
correctRate = list()
recallRate = list()


# æ„é€ åœç”¨è¯å­—å…¸
def CreateStopWords():
    global stopWords
    with codecs.open("./res/stop_words_ch.txt", 'r', encoding='ANSI') as stopFile:  # ç”¨ansiçš„ç¼–ç æ‰“å¼€æ–‡ä»¶
        for word in stopFile:
            stopWords[str(word.strip())] = 1  # å»é™¤é¦–å°¾ç©ºæ ¼


# é¢„å¤„ç†ï¼šå¯¹æ–‡æ¡£åšåˆ†è¯ã€å»åœç”¨è¯ç­‰å‡†å¤‡å·¥ä½œ
def Preprocess():
    # éå†æ¯ä¸€ä¸ªç±»åˆ«
    global wholeDF
    global wholeWF
    global stopWords

    countType = 6
    for curDir, subDir, files in os.walk("./res/originalTexts/6", topdown=True):  # è‡ªé¡¶å‘ä¸‹æ‰«æç›®å½•ä¸‹æ‰€æœ‰å­ç›®å½•å’Œæ–‡ä»¶
        if files:  # å½“å‰ç›®å½•ä¸‹æœ‰æ–‡ä»¶
            # countType += 1  # å½“å‰éå†çš„ç±»å‹
            cutTextDir = os.path.join("./res/cutTexts", f"{countType}")  # åˆ†è¯åçš„æ–‡ä»¶çš„å­˜æ”¾ç›®å½•
            cleanDir = os.path.join("./res/cleanTexts", f"{countType}")  # ç§»é™¤éåè¯æ€§ä¸åœç”¨è¯çš„æ–‡ä»¶ç›®å½•
            # å­˜æ”¾ç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»ºä¸€ä¸ª
            if not os.path.exists(cutTextDir):
                os.makedirs(cutTextDir)
            if not os.path.exists(cleanDir):
                os.makedirs(cleanDir)

            typeWordFrequency = Counter()
            typeDocFrequency = dict()  # typeDocFrequency[word] = wordå‡ºç°çš„æ–‡æœ¬æ•°

            countFiles = 0  # éå†çš„å½“å‰ç±»å‹çš„æ–‡ä»¶æ•°
            # éå†æ¯ä¸€ä¸ªæ–‡ä»¶
            for curFileName in files:
                countFiles += 1
                writeFilesName = f"{countFiles}.txt"
                if countFiles > 10000:
                    break
                curReadPath = os.path.join(curDir, curFileName)  # å½“å‰è¯»å–æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
                # è¯»å–åŸå§‹æ–‡ä»¶
                with codecs.open(f"{curReadPath}", 'r', encoding='utf-8') as curReadFile:
                    oriContent = curReadFile.read()  # åˆ†å‰²å‰çš„åŸå§‹æ–‡æœ¬å†…å®¹
                    words = posseg.cut(oriContent)  # åˆ†è¯å¹¶æ ‡æ³¨è¯æ€§
                    curWritePath = os.path.join(cutTextDir, writeFilesName)  # è¦å†™å…¥çš„æ–‡ä»¶çš„åœ°å€
                    # æŠŠåˆ†è¯ç»“æœå†™å…¥æ–‡ä»¶
                    wordList = list()
                    with codecs.open(f"{curWritePath}", 'w', encoding='utf-8') as curWriteFile:
                        for curWord in words:
                            wordStr = str(curWord.word)
                            flagStr = str(curWord.flag)
                            writeData = f"{str(curWord.word)} {str(curWord.flag)}\n"
                            writeData.encode("utf-8")
                            curWriteFile.write(writeData)
                            # è¿‡æ»¤éåè¯å’Œåœæ­¢è¯
                            if 'n' in flagStr and flagStr != "nr" and wordStr not in stopWords:  # è¿‡æ»¤
                                wordList.append(wordStr)
                                if wordStr in typeDocFrequency:  # æ›´æ–°è¯¥è¯è¢«å¼•ç”¨çš„æ–‡æ¡£æ•°
                                    typeDocFrequency[wordStr] += 1
                                else:
                                    typeDocFrequency[wordStr] = 1

                    # è¿‡æ»¤éåè¯å’Œåœæ­¢è¯å’Œäººå
                    cleanFilePath = os.path.join(cleanDir, writeFilesName)  # è¦å†™å…¥çš„æ–‡ä»¶çš„åœ°å€
                    with codecs.open(f"{cleanFilePath}", 'w', encoding='utf-8') as cleanFile:
                        listSize = len(wordList)
                        for i in range(0, listSize):
                            cleanFile.write(f"{wordList[i]} ")  # ç¬¦åˆæ¡ä»¶çš„è¯å†™å…¥æ–‡ä»¶

                    fileWordFrequency = Counter(wordList)  # è¯¥æ–‡ä»¶ä¸­å«æœ‰çš„åˆæ³•è¯
                    if countFiles <= 5000:  # å–å‰5000ä¸ªè®­ç»ƒ
                        typeWordFrequency.update(wordList)  # è®¡ç®—è¯¥ç±»å‹è¯é¢‘
                    else:  # å5000ä¸ªæµ‹è¯•é›†ï¼Œè®°å½•æ¯ä¸ªé›†åˆè¯é¢‘æœ€é«˜çš„400ä¸ªè¯
                        testFilePath = f"./res/TestFiles/{countType}/{writeFilesName}"
                        with codecs.open(testFilePath, 'w', encoding='utf-8') as cleanTestFile:
                            maxIndex = 400
                            if len(fileWordFrequency) < 400:
                                maxIndex = len(fileWordFrequency)
                            temp = fileWordFrequency.most_common(maxIndex)
                            for i in range(0, maxIndex):
                                cleanTestFile.write(f"{temp[i][0]}\n")
            wholeDF.append(typeDocFrequency)
            wholeWF.append(typeWordFrequency)


# æ„é€ wholeæ•°ç»„ï¼Œç»Ÿè®¡æ¯ä¸ªç±»åˆ«ä¸­çš„è¯é¢‘å’Œè¢«å¼•ç”¨çš„æ–‡æ¡£æ•°
def ReadTrainCleanFile():
    global wholeDF  # æŸä¸ªç±»åˆ«çš„å¹²å‡€è¯é¢‘æ•°å­—å…¸
    global wholeWF  # æŸä¸ªç±»åˆ«çš„å¹²å‡€è¯åœ¨è¯¥ç±»åˆ«ä¸­å‡ºç°çš„æ–‡æœ¬æ•°
    global stopWords

    for curType in range(1, 11):  # éå†æ¯ä¸ªç±»åˆ«çš„æ–‡æœ¬
        typeWordFrequency = Counter()  # ç»Ÿè®¡å½“å‰çš„ç±»åˆ«æ¯ä¸ªè¯å‡ºç°çš„é¢‘ç‡
        typeDocFrequency = dict()  # typeDocFrequency[word] = wordå‡ºç°çš„æ–‡æœ¬æ•°

        cleanDir = os.path.join("./res/cleanTexts", f"{curType}")  # ç§»é™¤éåè¯æ€§ä¸åœç”¨è¯çš„æ–‡ä»¶ç›®å½•
        for curFile in range(1, 5001):
            fileCounter = Counter()
            cleanFilePath = os.path.join(cleanDir, f"{curFile}.txt")  # è¦è¯»å…¥çš„å¹²å‡€è¯æ–‡ä»¶çš„åœ°å€
            with codecs.open(f"{cleanFilePath}", 'r', encoding='utf-8') as cleanFile:
                wordList = cleanFile.read().split()
                typeWordFrequency.update(wordList)  # è®¡ç®—è¯¥ç±»å‹è¯é¢‘
                fileCounter.update(wordList)
            for wordStr in fileCounter.keys():
                if wordStr in typeDocFrequency:  # æ›´æ–°è¯¥è¯è¢«å¼•ç”¨çš„æ–‡æ¡£æ•°
                    typeDocFrequency[wordStr] += 1
                else:
                    typeDocFrequency[wordStr] = 1
        wholeDF.append(typeDocFrequency)
        wholeWF.append(typeWordFrequency)


# è¯»å–åˆ†è¯åçš„æ–‡ä»¶å¹¶è¿‡æ»¤ï¼Œè¿‡æ»¤åå†™å…¥cleanæ–‡ä»¶
def ReadCutFile():
    for curType in range(1, 11):
        cutTextDir = os.path.join("./res/cutTexts", f"{curType}")  # åˆ†è¯åçš„æ–‡ä»¶çš„å­˜æ”¾ç›®å½•
        cleanDir = os.path.join("./res/cleanTexts", f"{curType}")  # ç§»é™¤éåè¯æ€§ä¸åœç”¨è¯çš„æ–‡ä»¶ç›®å½•
        for curFile in range(1, 10001):
            cutFilePath = os.path.join(cutTextDir, f"{curFile}.txt")  # æ–‡ä»¶çš„åœ°å€
            cleanWords = list()
            with codecs.open(f"{cutFilePath}", 'r', encoding='utf-8') as cutFile:
                wordList = cutFile.read().split('\n')  # è¯¥æ–‡æœ¬å¯¹åº”çš„æ‰€æœ‰åˆ†è¯
                for i in range(0, len(wordList)):
                    curPair = wordList[i].split(' ')
                    if len(curPair) == 2:
                        wordStr = curPair[0]  # å½“å‰éå†çš„è¯
                        flagStr = curPair[1]  # å½“å‰éå†çš„è¯çš„å±æ€§
                        # è¿‡æ»¤éåè¯å’Œäººåå’Œåœæ­¢è¯
                        if 'n' in flagStr and flagStr != "nr" and wordStr not in stopWords:
                            cleanWords.append(wordStr)  #ç¬¦åˆæ¡ä»¶çš„è¯æ ‡è®°ä½å¹²å‡€çš„è¯

            cleanFilePath = os.path.join(cleanDir, f"{curFile}.txt")  # è¦å†™å…¥çš„æ–‡ä»¶çš„åœ°å€
            with codecs.open(f"{cleanFilePath}", 'w', encoding='utf-8') as cleanFile:
                listSize = len(cleanWords)
                for i in range(0, listSize):
                    cleanFile.write(f"{cleanWords[i]} ")  # ç¬¦åˆæ¡ä»¶çš„è¯å†™å…¥æ–‡ä»¶


# è¯»å–å¹²å‡€çš„æµ‹è¯•è¯é›†ï¼Œæå–ç‰¹å¾è¯
def CleanTestFile():
    for curType in range(1, 11):
        for curFile in range(5001, 10001):
            cleanFilePath = f"./res/cleanTexts/{curType}/{curFile}.txt"
            testFilePath = f"./res/TestFiles/{curType}/{curFile}.txt"
            fileWordFrequency = Counter()

            with codecs.open(f"{cleanFilePath}", 'r', encoding='utf-8') as cleanFile:  # read
                wordList = cleanFile.read().split()
                fileWordFrequency.update(wordList)  # è®¡ç®—è¯¥ç±»å‹è¯é¢‘

            with codecs.open(testFilePath, 'w', encoding='utf-8') as cleanTestFile:  # write
                maxIndex = 400
                if len(fileWordFrequency) < 400:
                    maxIndex = len(fileWordFrequency)
                temp = fileWordFrequency.most_common(maxIndex)
                for i in range(0, maxIndex):
                    cleanTestFile.write(f"{temp[i][0]}\n")


# æå–æ¯ä¸ªç±»åˆ«çš„è®­ç»ƒé›†ä¸­çš„ç‰¹å¾è¯,æœ€å¤šå–400ä¸ª
def ExtractFeature(mode):
    global wholeWF
    for curType in range(1, 11):  # å…±10ä¸ªåˆ†ç±»
        maxIndex = 400
        if len(wholeWF[curType]) < 400:
            maxIndex = len(wholeWF[curType])

        if mode == "CHI":  # é€‰å¡æ–¹å€¼æœ€å¤§çš„åšç‰¹å¾è¯
            wordCHI = ComputeCHI(curType)
            sortWordChi = sorted(wordCHI.items(), key=lambda x: x[1], reverse=True)[:maxIndex]  # æŒ‰chiçš„å€¼é™åºæ’åˆ—
            ComputeWordP(curType, sortWordChi, maxIndex)
        elif mode == "TF/IDF":
            pass
        elif mode == "DF":  # é€‰è¯é¢‘æœ€é«˜çš„åšç‰¹å¾è¯
            ComputeWordP(curType, wholeWF[curType].most_common(maxIndex), maxIndex)


# è®¡ç®—ç‰¹å¾è¯åœ¨æŸä¸ªç±»åˆ«ä¸­çš„ä¼¼ç„¶åº¦P(word|type) ï¼Œ é‡‡ç”¨æ‹‰æ™®æ‹‰æ–¯è¿›è¡Œå¹³æ»‘å¤„ç†
def ComputeWordP(curType, wordList, maxIndex):
    global featureWords
    global wholeWF
    feature = dict()
    totalWords = maxIndex  # æœ¬ç±»å‹æ–‡æ¡£ä¸­ç‰¹å¾è¯çš„æ€»ä¸ªæ•°ï¼ˆç‰¹å¾è¯æœ€å¤šå–400ä¸ªï¼‰, é‡‡ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼Œåˆ†æ¯åŠ ä¸Šè¯æ•°
    for i in range(0, maxIndex):
        curWord = wordList[i][0]  # wordList[i]æ˜¯ä¸€ä¸ªäºŒå…ƒå…ƒç»„ï¼ˆwordï¼Œvalueï¼‰
        totalWords += wholeWF[curType][curWord]
    totalWordsCount.append(totalWords)

    # é‡‡ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼Œå…¬å¼å¦‚ä¸‹ï¼š
    # P(word|type) = ï¼ˆwordåœ¨typeçš„æ‰€æœ‰æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•° + 1ï¼‰/ ï¼ˆæœ¬ç±»å‹æ–‡æ¡£ä¸­ç‰¹å¾è¯çš„æ€»ä¸ªæ•°+ç‰¹å¾è¯æ•°ï¼‰
    featureFilePath = f"./res/featureWords/{curType}.txt"
    with codecs.open(f"{featureFilePath}", 'w', encoding='utf-8') as featureFile:
        featureFile.write(f"{totalWords}\n")
        for i in range(0, maxIndex):
            curWord = wordList[i][0]
            p = float(wholeWF[curType][curWord] + 1) / float(totalWords)  # è®¡ç®—P(word|type)
            feature[curWord] = p
            featureFile.write(f"{str(curWord)} {p}\n")
    featureWords.append(feature)


def ComputeCHI(curType):
    # ğ‘‹2(word, type) = ğ‘ Ã— (ğ´ğ· âˆ’ ğµğ¶)^2 /(ğ´ + ğ¶)(ğµ + ğ·)(ğ´ + ğµ)(ğ¶ + ğ·)
    # N = 10 * 5000, è®­ç»ƒé›†æ–‡æœ¬æ€»æ•°
    # A ä¸ºè®­ç»ƒé›†ä¸­ç±»åˆ«typeåŒ…å«è¯è¯­wordçš„æ–‡æ¡£æ•°, C ä¸ºç±»åˆ«typeä¸­ä¸å«wordçš„æ–‡æ¡£æ•°ï¼ŒA+Cä¸º5000
    # B ä¸ºè®­ç»ƒé›†ä¸­ç±»åˆ«ä¸ä¸ºtypeåŒ…å«è¯è¯­wordçš„æ–‡æ¡£æ•°, C ä¸ºç±»åˆ«ä¸ä¸ºtypeä¸­ä¸å«wordçš„æ–‡æ¡£æ•°ï¼ŒB+D = 5000 * 9
    # æ‰€ä»¥å¡æ–¹è®¡ç®—å¯ä»¥ç®€åŒ–æˆ (ğ´ğ· âˆ’ ğµğ¶)/(ğ´ + ğµ)(ğ¶ + ğ·)
    global wholeDF
    wordCHI = dict()
    for word, A in wholeDF[curType].items():  # éå†å½“å‰ç±»å‹çš„æ‰€æœ‰è®­ç»ƒæ–‡æ¡£ä¸­åŒ…å«çš„åˆæ³•è¯
        C = 5000 - A
        B = ComputeB(word, curType)
        D = 45000 - B
        wordCHI[word] = (A * D - B * C) * (A * D - B * C) / float(A + B) / float(C + D)
    return wordCHI


# B ä¸ºè®­ç»ƒé›†ä¸­ç±»åˆ«ä¸ä¸ºtypeåŒ…å«è¯è¯­wordçš„æ–‡æ¡£æ•°
def ComputeB(word, exceptType):
    global wholeDF
    countB = 0
    for curType in range(1, 11):
        if curType == exceptType:
            continue
        else:
            if word in wholeDF[curType]:
                countB += wholeDF[curType][word]
    return countB


def Init():
    global TP  # TP[type] è¡¨ç¤ºåˆ†ç±»å™¨è¯†åˆ«æ­£ç¡®ï¼Œåˆ†ç±»å™¨è®¤ä¸ºè¯¥æ ·æœ¬ä¸ºtype
    global FP  # FP[type] åˆ†ç±»å™¨è¯†åˆ«ç»“æœé”™è¯¯ï¼Œåˆ†ç±»å™¨è®¤ä¸ºè¯¥æ ·æœ¬ä¸ºtype, å®é™…ä¸Šè¯¥æ ·æœ¬ä¸æ˜¯type
    global FN  # FN[type] åˆ†ç±»å™¨è¯†åˆ«ç»“æœé”™è¯¯ï¼Œåˆ†ç±»å™¨è®¤ä¸ºè¯¥æ ·æœ¬ä¸æ˜¯type, å®é™…ä¸Šè¯¥æ ·æœ¬æ˜¯type
    global correctRate
    global recallRate
    for curType in range(0, 11):
        TP.append(0)  # TP[curType] = 0
        FP.append(0)
        FN.append(0)
        correctRate.append(0.0)
        recallRate.append(0.0)
        resultMatrix.append(list())
        for i in range(0, 11):
            resultMatrix[curType].append(0)


def NBClassify():
    global TP  # TP[type] è¡¨ç¤ºåˆ†ç±»å™¨è¯†åˆ«æ­£ç¡®ï¼Œåˆ†ç±»å™¨è®¤ä¸ºè¯¥æ ·æœ¬ä¸ºtype
    global FP  # FP[type] åˆ†ç±»å™¨è¯†åˆ«ç»“æœé”™è¯¯ï¼Œåˆ†ç±»å™¨è®¤ä¸ºè¯¥æ ·æœ¬ä¸ºtype, å®é™…ä¸Šè¯¥æ ·æœ¬ä¸æ˜¯type
    global FN  # FN[type] åˆ†ç±»å™¨è¯†åˆ«ç»“æœé”™è¯¯ï¼Œåˆ†ç±»å™¨è®¤ä¸ºè¯¥æ ·æœ¬ä¸æ˜¯type, å®é™…ä¸Šè¯¥æ ·æœ¬æ˜¯type

    # æ¯ä¸€ä¸ªç±»åˆ«æœ‰5000ä¸ªæµ‹è¯•é›†
    for curTestType in range(1, 11):
        for curTestFile in range(5001, 10001):
            testFilePath = f"./res/TestFiles/{curTestType}/{curTestFile}.txt"
            testWordList = GetTestWords(testFilePath)  # è·å–æµ‹è¯•é›†çš„åˆæ³•è¯åˆ—è¡¨

            # æµ‹è¯•æ¯ä¸€ç§ç±»åˆ«çš„æ¦‚ç‡
            resultType = ComputeNBType(testWordList)
            resultMatrix[curTestType][resultType] += 1
            # å°†ç»“æœä¸æ­£ç¡®ç±»åˆ«è¿›è¡Œæ¯”è¾ƒ
            if resultType == curTestType:  # æˆåŠŸ
                TP[resultType] += 1
            else:  # å¤±è´¥
                FP[resultType] += 1
                FN[curTestType] += 1


# è¯„ä¼°åˆ†ç±»å™¨å¹¶æ‰“å°ç»“æœ
def EvaluateClassifier():
    # æ­£ç¡®ç‡ = TP / (TP + FP)
    # å¬å›ç‡ = TP / (TP + FN)
    global TP  # TP[type] è¡¨ç¤ºåˆ†ç±»å™¨è¯†åˆ«æ­£ç¡®ï¼Œåˆ†ç±»å™¨è®¤ä¸ºè¯¥æ ·æœ¬ä¸ºtype
    global FP  # FP[type] åˆ†ç±»å™¨è¯†åˆ«ç»“æœé”™è¯¯ï¼Œåˆ†ç±»å™¨è®¤ä¸ºè¯¥æ ·æœ¬ä¸ºtype, å®é™…ä¸Šè¯¥æ ·æœ¬ä¸æ˜¯type
    global FN  # FN[type] åˆ†ç±»å™¨è¯†åˆ«ç»“æœé”™è¯¯ï¼Œåˆ†ç±»å™¨è®¤ä¸ºè¯¥æ ·æœ¬ä¸æ˜¯type, å®é™…ä¸Šè¯¥æ ·æœ¬æ˜¯type
    global correctRate
    global recallRate

    totalCorrect = 0.0
    totalRecall = 0.0

    # è®¡ç®—æ¯ç±»æ­£ç¡®ç‡ã€å¬å›ç‡
    for curType in range(1, 11):
        correctRate[curType] = float(TP[curType]) / float(TP[curType] + FP[curType])
        recallRate[curType] = float(TP[curType]) / float(TP[curType] + FN[curType])
        print(f"Type {curType}: æ­£ç¡®ç‡ï¼š{correctRate[curType]}  å¬å›ç‡ï¼š{recallRate[curType]}\n")
        totalCorrect += correctRate[curType]
        totalRecall += recallRate[curType]
    # è®¡ç®—æ€»ä½“æ­£ç¡®ç‡ã€å¬å›ç‡
    totalCorrect /= 10
    totalRecall /= 10
    print(f"Total: æ­£ç¡®ç‡ï¼š{totalCorrect}  å¬å›ç‡ï¼š{totalRecall}\n")

    print(f"{str(1): >12}{str(2): >6}{str(3): >6}{str(4): >6}{str(5): >6}{str(6): >6}{str(7): >6}{str(8): >6}{str(9): >6}{str(10): >6}\n")
    for i in range(1, 11):
        outputStr = f"{str(i).rjust(6, ' ')}"
        for j in range(1, 11):
            outputStr += f"{str(resultMatrix[i][j]).rjust(6, ' ')}"
        print(f"{outputStr}\n")


def GetTestWords(filePath):
    wordList = list()
    with codecs.open(filePath, 'r', encoding='utf-8') as curReadFile:
        for word in curReadFile:
            curWord = str(word.strip())
            wordList.append(curWord)
    return wordList


def ReadFeature():
    global featureWords
    global totalWordsCount
    for curType in range(1, 11):
        featureWords.append(dict())
        featureFilePath = f"./res/featureWords/{curType}.txt"
        with codecs.open(f"{featureFilePath}", 'r', encoding='utf-8') as featureFile:
            wordList = featureFile.read().split('\n')
            totalWordsCount.append(int(wordList[0]))
            for i in range(1, len(wordList)):
                curPair = wordList[i].split(' ')
                if len(curPair) == 2:
                    featureWords[curType][curPair[0]] = float(curPair[1])


# åˆ©ç”¨æœ´ç´ è´å¶æ–¯è¿›è¡Œåˆ†ç±»
def ComputeNBType(testWordsList):
    # å‡è®¾æ¯ç§ç±»åˆ«æ–‡æ¡£çš„æ¦‚ç‡éƒ½ç›¸åŒï¼ŒP(type) = 1/10
    # NB(type) = P(testFile|type)*P(type)
    # è®°testFileé‡Œçš„ç‰¹å¾è¯åˆ†åˆ«ä¸ºw1,w2...wnï¼Œåˆ™
    # P(testFile|type) = P(w1|type)*P(w2|type)*...*P(wn|type), å®é™…è®¡ç®—ä¸­ç”¨logçš„åŠ æ³•ä»£æ›¿ä¹˜æ³•

    global totalWordsCount  # totalWordsCount[type] = typeç±»å‹å¯¹åº”çš„æ€»è¯æ•°
    global featureWords  # featureWords[type]=typeç±»å‹å¯¹åº”çš„ç‰¹å¾è¯
    resultType = 1
    maxPossibility = -10000000.0
    testSize = len(testWordsList)
    for curTrainType in range(1, 11):  # æµ‹è¯•æ¯ä¸€ç§ç±»å‹çš„NBæ¦‚ç‡
        trainTypeP = 0.0
        for i in range(0, testSize):  # éå†testFileé‡Œçš„ç‰¹å¾è¯
            word = testWordsList[i]
            if word in featureWords[curTrainType]:  # å¦‚æœæµ‹è¯•é›†çš„ç‰¹å¾è¯åœ¨ç±»åˆ«ç‰¹å¾è¯é‡Œ
                trainTypeP += math.log2(featureWords[curTrainType][word])
            else:  # ä¸åœ¨ç±»åˆ«ç‰¹å¾è¯é‡Œï¼Œå³è¯¥è¯åœ¨è¿™ç±»æ–‡æ¡£ä¸­é¢‘æ•°ä¸º0
                # æ ¹æ®æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼Œ P(word|type) = (0 + 1) / totalWordsCount[type]
                trainTypeP -= math.log2(totalWordsCount[curTrainType])  # math.log2(1) = 0ï¼Œçœç•¥
        if trainTypeP > maxPossibility:  # å¾—åˆ°æœ€å¤§çš„å€¼çš„typeå³ä¸ºåˆ†ç±»ç»“æœ
            maxPossibility = trainTypeP
            resultType = curTrainType
    return resultType


def SVM():
    pass


if __name__ == '__main__':
    Init()
    CreateStopWords()
    # Preprocess()  # ç¬¬ä¸€æ¬¡å¤„ç†æ•°æ®é›†ç”¨è¿™ä¸ª

    # ReadTrainCleanFile()  # æ„é€ wholeæ•°ç»„
    # ExtractFeature("CHI")
    ReadFeature()
    startClassify = time.time()
    NBClassify()
    endClassify = time.time()
    print(f"æµ‹è¯•æ—¶é—´ï¼š{endClassify - startClassify}")
    EvaluateClassifier()
